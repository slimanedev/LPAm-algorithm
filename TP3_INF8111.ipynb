{"cells":[{"cell_type":"markdown","metadata":{"id":"5fHWJhwOXGzh"},"source":["# INF8111 - Fouille de données / Data Mining\n","## Automne 2021 - TP3 - Fouille de réseaux sociaux / Mining of social networks\n","### Membres de l'équipe / Team members\n","- Slimane Aglagal (2103355) 1\n","- Colline Blanc (2096695) 2\n","- Baptiste Pauletto (2096684) 3\n"]},{"cell_type":"markdown","metadata":{"id":"TmGvqtSVgfXi"},"source":["## Instructions de remise / Submission\n","Vous devez remettre dans la boîte de remise sur moodle:\n","\n","1. ce fichier nommé TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n","\n","**N.B**: Assurez-vous que tous les résultats soient lisibles lorsque le notebook est ouvert.\n","\n","Tout devra être remis avant le **5 décembre 2021 à 23h59**. Tout travail en retard sera pénalisé d’une valeur de 10\\% par jour ouvrable de retard.\n","\n","## Barème\n","\n","Partie 1: 10 points\n","\n","Partie 2: 4 points\n","\n","Partie 3: 6 points\n","\n","Pour un total de 20 points.\n","\n","\n","---\n","\n","## Submission\n","You must put back in the submission box on moodle:\n","\n","1. this file renamed TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n","\n","**N.B**: Make sure that all results are there when you open your notebook.\n","\n","Everything must be submitted before **December 5th 2021 à 23h59**. Any late work will be penalized with a value of 10% per open day of delay.\n","\n","## Barème\n","Part 1: 10 points\n","\n","Part 2: 8 points\n","\n","Part 3: 6 points\n","\n","For a total of 20 points on 20 points.\n"]},{"cell_type":"markdown","metadata":{"id":"P1et8f3nXGzk"},"source":["## Réseaux sociaux / Social Networks\n","Les réseaux sociaux occupent une grande partie de la vie humaine. Chaque personne appartient tout le long de sa vie à différentes communautés. Avec le rassemblage de ces informations sur les différentes plateformes en ligne de réseaux sociaux, les analystes de données ont voulu exploiter ces données. C'est un domaine relativement nouveau qui est en pleine croissance avec plusieurs impacts sur plusieurs aspects tels que la publicité et les systèmes de recommandation. \n","\n","### But\n","Le but de ce TP est de vous donner un aperçu de l'analyse d'un réseau social.\n","\n","Dans la première partie, vous implémenterez un algorithme de détection de communautés dans un réseau social nommé LPAm+. Cet algorithme a été proposé par [X. Liu et T. Murata en 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n","\n","Dans la deuxième partie, vous trouverez les personnes avec le plus d'influence dans leur réseau social. \n","\n","Pour les deux parties, nous vous fournissons les CSV contenant les réseaux sociaux à analyser.\n","\n","\n","---\n","\n","## Social networks\n","Social networks are a major component of the human life. Each person belongs throughout their life to different communities. With the aggregation of information on various online social media platforms, data analysts were interested in exploiting its data. It is a relatively new field that is growing with impacts on several aspects such as advertising and recommendation systems.\n","\n","\n","### Goal\n","The purpose of this lab is to give you an overview of social network analysis.\n","\n","In the first part, you will implement an algorithm for detecting communities in a social network called LPAm+. This algorithm was proposed by [X. Liu and T. Murata in 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n","\n","In the second part, you will find the people with the most influence in their social network.\n","\n","For both parties, we provide you with all the csv containing the social networks to be analysed."]},{"cell_type":"markdown","metadata":{"id":"517JwzlPXGzp"},"source":["# 1. LPAm+ (10 points)\n","\n","## Détection de communauté\n","La détection de communauté dans un réseau social est une manipulation fréquente lors de l'analyse d'un réseau. Une méthode de clustering est utilisée pour rassembler les personnes dans des communautés selon les liens entre eux. \n","\n","## LPAm+\n","Dans cette partie, vous implémenterez l'algorithme LPAm+ pour détecter les communautés parmi les personnages de Games of Thrones. Vous devez utiliser les CSV *nodes* et *edges* pour cela. \n","\n","Cet algorithme consiste à propager les étiquettes dans le réseau selon une règle d'évaluation optimisant la modularité du réseau. Lorsque l'algorithme atteint un optimum local, il regarde s'il peut combiner deux communautés pour augmenter la modularité du réseau. L'algorithme choisit toujours la combinaison la plus avantageuse. Si une combinaison est trouvée, la propagation des étiquettes est refaite. L'algorithme continue tant qu'elle peut améliorer la modularité. Vous pouvez lire l'article mentionné plus haut pour plus de détails, mais cela n'est pas nécessaire puisque vous allez être guidé tout le long du TP. \n","\n","Pour faciliter la représentation du réseau, nous vous proposons d'utiliser le package networkx. La documentation est disponible [ici](https://networkx.github.io/documentation/stable/tutorial.html).\n","\n","\n","\n","---\n","\n","# 1. LPAm+ (10 points)\n","\n","\n","## Community detection\n","Community detection in a social network is a frequent manipulation when analysing a network. A clustering method is used to bring people together in communities according to the links between them.\n","\n","\n","## LPAm+\n","In this part, you will implement the LPAm+ algorithm to detect the communities among the characters of Games of Thrones. You must use the nodes and edges csv for this.\n","\n","This algorithm consists in propagating the labels in the network according to an evaluation rule optimizing the modularity of the network. When the algorithm reaches a local optimum, it checks whether it can combine two communities to increase the modularity of the network. The algorithm always chooses the most advantageous combination. If a combination is found, the propagation of the labels is redone. The algorithm continues until it is no longer able to increase modularity. You can read the article mentioned above for more details, but you do not need to, as you will be guided throughout the TP.\n","\n","\n","To help you represent a network, we suggest that you use the networkx package.You can read more about the package [here](https://networkx.github.io/documentation/stable/tutorial.html)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9DGyw323Srh0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting networkx\n","  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 3.2 MB/s \n","\u001b[?25hInstalling collected packages: networkx\n","Successfully installed networkx-2.6.3\n","\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n","You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["# vous pouvez bien sûr utiliser anaconda pour installer les packages\n","#!pip install --user numpy\n","#!pip install --user pandas\n","#!pip install --user matplotlib\n","!pip install --user networkx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SK6OhgIoSrh5"},"outputs":[],"source":["import numpy as np\n","import networkx as nx\n","import random\n","import itertools\n","import math\n","\n","\n","class LPAmPlus:\n","    \"\"\"\n","    Contructor\n","    \"\"\"\n","\n","    def __init__(self, graph):\n","        \"\"\"\n","        graph gives the graph on which the algorithm will be applied;\n","        \"\"\"\n","        self.graph = graph\n","\n","\n","        \"\"\"\n","        Assign a label to each node\n","        \"\"\"\n","        n=self.graph.number_of_nodes()\n","        for u in range(n-1):\n","          self.graph.node[u]['label']=self.graph.node[u]['Community']\n","\n","\n","        \"\"\"\n","        labels gives all the communities present in the network\n","        \"\"\"\n","        labels=[]\n","        for u in range(n-1):\n","          if self.graph.node[u]['label'] not in labels:\n","            labels.append(self.graph.node[u]['label'])\n","        self.labels = labels\n","\n","       \n","\n","    \"\"\"\n","    Term to optimize when replacing labels\n","    \"\"\"\n","\n","    def label_evaluation(self, current_node, new_label):\n","        #Matrice B\n","        B_matrix=nx.linalg.modularity_matrix(self.graph)\n","        #Iteration pour le calcul de la somme\n","        delta=0\n","        n=self.graph.number_of_nodes()\n","        for v in range(n-1):\n","          if v!=current_node and self.graph.nodes[v]['label']==new_label:\n","            delta+=B_matrix[v,current_node]\n","        return delta\n","\n","    \"\"\"\n","    Function to choose the new label for a node\n","    \"\"\"\n","\n","    def update_label(self, current_node):\n","        #initialisation du term à optimiser et le nouveau label\n","        term=self.label_evaluation(current_node,self.graph.nodes[current_node]['label'])\n","        new_label=self.graph.nodes[current_node]['label']\n","        for label in self.labels:\n","          new_term=self.label_evaluation(current_node,label)\n","          if new_term>term:\n","            term=new_term\n","            new_label=label\n","        self.graph.nodes[current_node]['label']=new_label\n","        pass\n","\n","    def modularity(self):\n","        #Matrice B\n","        B_matrix=nx.linalg.modularity_matrix(self.graph)\n","        #Nombre d'arretes\n","        m=self.graph.number_of_edges()\n","        #Iteration pour le calcul de la somme\n","        delta=0\n","        n=self.graph.number_of_nodes()\n","        for u in range(n-1):\n","          for v in range(n-1):\n","            if v!=u and self.graph.nodes[u]['label']==self.graph.nodes[v]['label']:\n","              delta+=B_matrix[v,u]\n","\n","        modularity=delta/(2*m)\n","        return modularity\n","    \n","    \"\"\"\n","    Function that applies the LPAm algorithm on the network\n","    \"\"\"\n","\n","    def LPAm(self):\n","        #TODO\n","   \n","    \"\"\"\n","    Function that find which communities to combine and combine them\n","    \"\"\"\n","    def merge_communities(self):\n","        #TODO\n","        return False\n","    \n","    \n","    \"\"\"\n","    Function that applies the LPAm+ algorithm on the network\n","    \"\"\"\n","\n","    def find_communities(self):\n","        #TODO\n","\n","            \n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"Yv83c1sWXGzq"},"source":["### 1.1 Dataset (0.5 point)\n","\n","Nous vous avons fourni les CSV pour toutes les saisons de Games of Thrones. Vous devez maintenant représenter ces réseaux en utilisant les deux CSV fournis pour chaque saison: un pour les sommets et un pour les arêtes. \n","\n","\n","#### Implémentation\n","1. Implémentez  la fonction  *`load_unweighted_network`*. Cette fonction retourne le réseau non dirigé et sans poids.\n","\n","Pour faciliter la représentation du réseau, nous vous proposons d'utiliser le package networkx. La documentation est disponible [ici](https://networkx.github.io/documentation/stable/tutorial.html).\n","\n","Utilisez la fonction `test_load` pour vérifier votre implémentation de la fonction. Ce test utilise un petit toy dataset. Vous devriez avoir quelque chose de similaire (data/picture.png):\n","![title](data/picture.png)\n","\n","\n","---\n","We have provided you with the csv for all the seasons of Games of Thrones. You must now represent each of those networks in code using two csv for each season: the one for the nodes and the one for the edges.\n","\n","\n","#### Implementation\n","1. Implement the function *`load_unweighted_network`*. This function returns a undirected and unweighted graph.\n","\n","To help you represent a network, we suggest that you use the networkx package.You can read more about the package [here](https://networkx.github.io/documentation/stable/tutorial.html).\n","\n","Use the function `test_load` to verify your implementation of the function. This test use a toy dataset. You should obtain a result similar to this (data/picture.png):\n","![title](data/picture.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wGC7MC3Srh-"},"outputs":[],"source":["import csv\n","import pandas as pd\n","import networkx as nx\n","\n","def load_unweighted_network(node_csv, edge_csv):\n","  #Création d'un graphe vide et non dirigé\n","  graph_type=nx.Graph()\n","    \n","  #Lecture des files\n","  nodes = pd.read_csv(node_csv)\n","  edges = pd.read_csv(edge_csv)\n","\n","  #Ajout des edges au graphes \n","  network = nx.from_pandas_edgelist(edges, source='Source', target='Target', create_using=graph_type)\n","\n","  #Ajout des ID des nodes au graphes \n","  nodes_data = nodes.set_index('Id').to_dict('index').items()\n","  network.add_nodes_from(nodes_data)\n","  \n","  return network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEwBsNVsjvfs"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","def test_load():\n","    network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    nx.draw_networkx(network,font_color='white')\n","    plt.show()\n","\n","test_load()"]},{"cell_type":"markdown","metadata":{"id":"6UTIxaGIXGzz"},"source":["### 1.2  Modularité / Modularity (1 point)\n","\n","La modularité $Q$ du réseau est une mesure importante pour l'algorithme: elle permet de savoir si l'algorithme a atteint un optimum local. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n","\n","- m: le nombre d'arêtes\n","- l: l'étiquette du sommet\n","- u, v: des sommets dans le réseau\n","- B: la matrice de modularité où chaque élément vaut $A_{uv} - P_{uv}$\n","- $A_{uv}$: vaut 1 si il y une arête entre u et v sinon 0\n","- $P_{uv}$: la probabilité qu'il y ait une arête entre u et v selon le modèle nul  $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l_v)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n","\n","Elle peut aussi être définie comme: $$Q=\\sum_{t=1}^{N_c}\\left(\\frac{I_t}{m}-\\left(\\frac{D_t}{2m}\\right)^2\\right)$$\n","\n","- m: le nombre d'arêtes\n","- Nc: le nombre de communautés\n","- t: une communauté dans le réseau\n","- $I_t$: le nombre d'arêtes dans la communauté t c'est-à-dire que les deux sommets de l'arête appartiennent à t\n","- $D_t$: la somme des degrés de tous les sommets appartenant à t\n","\n","#### Implémentation\n","1. Implémentez  la fonction  `modularity`  dans LPAmPlus. Cette fonction retourne la modularité du réseau. Vous pouvez utiliser la fonction `linalg.modularity_matrix` de networkx pour calculer la matrice B. Prenez la définition présentée que vous voulez. **N.B:** Networkx permet d'ajouter du data sur les sommets pour garder des informations sur le node. Les nodes agissent comme des dictionnaires.\n","\n","Utilisez la fonction `test_modularity` pour vérifier votre implémentation de la fonction. Vous devriez obtenir une modularité d'environ 0.413.\n","\n","---\n","\n","The modularity $Q$ of the network is an important measure for the algorithm. The algorithm uses it to determine if it reached a local optimum or not. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n","\n","- m: number of edges\n","- l: node's label\n","- u, v: nodes in the graph\n","- B: modularity matrix where each element is $A_{uv} - P_{uv}$\n","- $A_{uv}$: is 1 if there is an edge between u and v else 0\n","- $P_{uv}$: probability that there is an edge between u and v following the null model $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l_v)$: Kronecker's delta, is 1 if labels are the same else 0\n","\n","The modularity can also be defined like this: $$Q=\\sum_{t=1}^{N_c}\\left(\\frac{I_t}{m}-\\left(\\frac{D_t}{2m}\\right)^2\\right)$$\n","\n","- m: number of edges\n","- Nc: the number of community in the graph\n","- t: a community in the graph\n","- $I_t$: the number of arc in the community t meaning all arcs that have both nodes in the community t\n","- $D_t$: the sum of degree of all the nodes in the community t\n","\n","#### Implementation\n","1. Implement the function `modularity` in the class LPAmPlus. This function returns the modularity of the network. You can use the function `linalg.modularity_matrix` from networkx to calculate B. You can implement whichever definition for the modularity. **N.B:** You can add data to nodes with Networkx to store information about the node. You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n","\n","Use the function `test_modularity` to test your implementation. You should have a modularity of 0.413."]},{"cell_type":"markdown","metadata":{"id":"t8iBYh5ohman"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7szMoEQSriF"},"outputs":[],"source":["def test_modularity():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    lpam = LPAmPlus(social_network)\n","    lpam.labels = [0, 1]\n","    for i in [0,1,2,3,4,5,6,7,8,9]:\n","        lpam.graph.nodes[i]['label'] = 0\n","    for i in [10,11,12,13,14,15]:\n","        lpam.graph.nodes[i]['label'] = 1\n","    print(\"Modularity: {}\".format(lpam.modularity()))\n","\n","test_modularity()"]},{"cell_type":"markdown","metadata":{"id":"vwxvqYj4XGzu"},"source":["### 1.3 Règle de modification des étiquettes / Updating rule for the labels (1.5 point)\n","\n","Comme mentionné plus haut, l'algorithme est fortement basé sur son optimisation de la modularité. Il vous est maintenant demandé d'implémenter le terme à optimiser. La nouvelle étiquette $l_x^{new}$ correspond à l'étiquette pour laquelle la somme donne la plus grande valeur.\n","$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n","\n","- n: le nombre de sommets\n","- m: le nombre d'arêtes\n","- l: une étiquette possible pour le sommet x\n","- x: le sommet qu'on évalue en ce moment\n","- u: un autre sommet dans le réseau (commence à 1, car on exclut le sommet x)\n","- B: la matrice de modularité où chaque élément vaut $A_{ux} - P_{ux}$\n","- $A_{ux}$: vaut 1 si il y une arête entre u et x sinon 0\n","- $P_{ux}$: la probabilité qu'il y ait une arête entre u et x selon le modèle nul  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n","\n","\n","#### Implémentation\n","1. Implémenter la fonction `label_evaluation`. Cette fonction retourne la valeur du terme à optimiser. Vous pouvez utiliser la fonction `linalg.modularity_matrix` de networkx pour calculer la matrice B. Il est normal qu'il y ait une ressemblance avec le calcul de la modularité selon la définition que vous avez prise. `new_label` correspond donc à un $l$ possible dans le terme.\n","2. Implémenter la fonction `update_label`. Cette fonction choisit la nouvelle étiquette pour le sommet actuel. En cas d'égalité, la fonction choisit une étiquette au hasard parmi les meilleurs. N'oubliez pas d'enlever les étiquettes désuètes du paramètre `labels`. **N.B:** Il est possible que la meilleure étiquette soit celle actuelle du sommet.\n","\n","Networkx permet d'ajouter du data sur les sommets. Les sommets sont des dictionnaires dans le graphe.\n","\n","---\n","\n","As mentioned above, the algorithm is strongly based on its optimization of modularity. You are now asked to implement the term to optimize. The new label $l_x^{new}$ corresponds to the label for which the sum gives the greatest value.\n","$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n","\n","- n: number of nodes\n","- m: number of edges\n","- l: a possible label for the node x\n","- x: current node being evaluated\n","- u: another node in the network (starts at 1, because we exclude the node x)\n","- B: modularity matrix where each element is $A_{ux} - P_{ux}$\n","- $A_{ux}$: is 1 if there is an edge between u and x else 0\n","- $P_{ux}$: the probability that there is an edge between u and x  following the null model  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n","- $\\delta(l_u,l)$: Kronecker's delta, is 1 if labels are the same else 0\n","\n","\n","#### Implementation\n","1. Implement the function `label_evaluation`. This function returns the value for the term to optimize. You can use the function `linalg.modularity_matrix` from networkx to calculate B. It is normal if there is a similarity with the modularity depending on the definition you took. `new_label` represent a possible $l$ in the term.\n","2. Implement the function `update_label`. This function chooses the new label for the current node. If there is more than one label with the max value, the function chooses randomly one amoung those. Don't forget to remove the unused labels from the `labels` attribute. **N.B:** The best label can be the node's current label. \n","\n","You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g5_N31uPXGz2"},"source":["### 1.4 LPAm (2 points)\n","\n","Vous pouvez maintenant implémenter l'algorithme LPAm. Cet algorithme est le prédécesseur de LPAm+ puisque LPAm+ a été crée pour contourner une faiblesse de LPAm.  LPAm est un algorithme de propapagation d'étiquettes basé sur la modularité. Il commence par donner une étiquette unique à chaque sommet. Il explore par la suite tous les sommets et change leur étiquette selon la fonction d'évaluation que vous avez implémentée plus tôt. L'algorithme continue la propagation d'étiquette à travers tous les sommets jusqu'à un optimun de la modularité.\n","\n","#### Implémentation\n","1. Ajouter les étiquettes initiales aux sommets du graphe dans la fonction `__init__`. Il faut que chaque sommet soit dans sa propre communauté au début de l'algorithme. Initialiser le paramètre `labels` pour qu'il contient la liste des étiquettes présentes dans le réseau.\n","\n","2. Implémenter l'algorithme LPAm dans la fonction `LPAm`. Assurez-vous de toujours augmenter la modularité lors de vos changements d'étiquettes. N'oubliez pas de garder le paramètre `labels` à jour à fur et à mesure lors de vos changements pour ne pas évaluer plusieurs fois la même étiquette.\n","\n","Utilisez la fonction `test_lpam` pour vérifier votre implémentation. Vous devriez finir avec une modularité d'environ 0.399 avec 4 communautés.\n","\n","---\n","\n","You can now implement the LPAm algorithm. This algorithm is the predecessor of LPAm+ since LPAm+ was created to overcome LPAm's weakness. LPAm is a label probagation algorithm based on modularity. It begins by giving a unique label to each node. It then explores all the nodes and changes their label according to the evaluation function that you implemented earlier. The algorithm continues until it can no longer improve the modularity of the network.\n","\n","#### Implementation\n","1. Add the initial labels to the nodes in the graph in the function `__init__`. Each nodes has to be in their own community in the beginning. Initialise `labels` with the current list of labels present in the graph.\n","\n","2. Implement the LPAm algorithm in the function`LPAm`. Make sure that all your labels changes improve the modularity. Don't forget to keep your `labels` parameter is kept up-to-date so that you dont evaluate the same label multiple times or unused labels.\n","\n","Use the function `test_lpam` to verify your implementation. You should have a modularity of 0.399 with 4 communities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2RfrO6PSriL"},"outputs":[],"source":["def test_lpam():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    lpam = LPAmPlus(social_network)\n","    lpam.LPAm()\n","    print(\"Modularity: {}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n","\n","test_lpam()"]},{"cell_type":"markdown","metadata":{"id":"cj5Ghd5jXGz6"},"source":["### 1.5 LPAm+ (2 point)\n","\n","Vous pouvez maintenant implémenter LPAm+ au complet. LPAm+ est une amélioration de LPAm. Lorsque LPAm tombe dans un optimum local, LPAm+ essaye de combiner deux communautés pour augmenter la modularité et ainsi sortir du optimum local. LPAm+ choisit la combinaison qui augmente le plus la modularité et recommence la propagation d'étiquette jusqu'au prochain optimum local où il va reessayer de combiner des communautés. L'algorithme continue jusqu'à qu'il ne peut plus augmenter la modularité.\n","\n","#### Implémentation\n","1. Implémentez  la fonction  `merge_communities`. Cette fonction regarde si combiner des communautés augmente la modularité et combine le meilleur choix. Elle retourne True si une combinaison a été faite sinon False (aucune combinaison augmente la modularité).\n","2. Implémenter `find_communities`. Cette fonction applique l'algorithme LPAm+ sur le réseau en utilisant les fonctions `LPAm` et `merge_communities`.\n","\n","Utilisez la fonction `test_lpam_plus` pour vérifier votre implémentation. Vous devriez finir avec une modularité d'environ 0.413 et 2 communautés.\n","\n","---\n","\n","You can now fully implement LPAm+. As said before LPAm+ is an amelioration of LPAm. The issue with LPAm is that it stops when it finds a local optimun. To prevent that, LPAm+ tries to combine two communities to increase modularity and escape the local optimun. LPAm+ chooses the combination that most increases modularity and restart the label's propagation until the next local optimum where it will try to combine two communities again. The algorithm continues until it can no longer increase modularity.\n","\n","#### Implementation\n","1. Implement the function  `merge_communities`. This function check if combining communities improve the modularity and combine the best choice. It returns True if a combinaison was made else False (no combination increase the modularity).\n","2. Implement the LPAM+ algorithm in the function `find_communities` using the fonctions `LPam` and `merge_communities`.\n","\n","Use the function `test_lpam_plus` to verify your implementation. You should end with a modularity of 0.413 and 2 communities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVSnUWQoSriP"},"outputs":[],"source":["def test_lpam_plus():\n","    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n","    lpam = LPAmPlus(social_network)\n","    lpam.find_communities()\n","    print(\"Modularity: {}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n","\n","test_lpam_plus()"]},{"cell_type":"markdown","metadata":{"id":"vODCJbRaXGz-"},"source":["### 1.6 GOT dataset (3 points)\n","\n","Rouler votre algorithme sur les données de Games of Thrones de chaque saison et comparer ce que vous obtenez et les vraies communautés. Le ground truth se trouve dans la colonne Community des csv. Des liens sont présents entre des personnages lorsque: \n","- Personnage A parle directement après Personnage B\n","- Personnage A parle de Personnage B\n","- Personnage C parle de Personnage A et Personnage B\n","- Personnage A et Personnage B font une action ensemble dans une scène (ex: quittent les lieux, A regarde B, sont assis à une table, etc)\n","- Personnage A et Personnage B apparaissent ensemble dans une scène\n","\n","Commencez par calculer le ARI (ajusted Rand index) de vos résultats. $$ ARI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n","\n","- n: le nombre de sommets\n","- TP: True positive soit le nombre de paires d'éléments qui se trouvent dans la même communauté dans vos résultats et dans le ground truth\n","- TN: True négative soit le nombre de paires d'éléments qui se trouvent dans des communautés différentes dans vos résultats et dans le ground truth\n","- FP: False positive soit le nombre de paires d'éléments qui se trouvent dans la même communauté dans vos résultats mais qui sont dans des communautés différentes dans le ground truth\n","- FN: False négative soit le nombre de paires d'éléments qui se trouvent dans des communautés différentes alors qu'ils sont dans la même communauté dans le ground truth\n","\n","\n","**N.B**: Ce n'est pas le nom des communautés que vous avez trouvé qui importante mais leur composition. Autrement dit, un TP est si le sommet a et le sommet b se trouve dans la même communauté dans vos résultats et dans le ground truth.\n","\n","\n","Répondez aux questions suivantes. Elles servent comme piste de réflexion pour votre analyse.\n","\n","- L'algorithme performe-t-il bien sur toutes les saisons ou pour certaines seulement? \n","- Expliquez pourquoi vous avez obtenu ces résultats en analysant la formation des communautés dans chaque saison. Quelles particularités favorisent des bons résultats? Quelles particularités nuisent à l'algorithme?\n","\n","Vous pouvez faire les manipulations que vous voulez pour mieux présenter vos résultats et mieux appuyer vos affirmations. \n","\n","---\n","\n","Run your algorithm over the Games of Thrones data from each season and compare what you get and the real communities. The ground truth is found in the Community column in the csv. Links are found between characters A and B when:\n","- Character A talks directly after Character B\n","- Character A talks about Character B\n","- Character C talks about Character B and A\n","- Character A and Character B does an action together in a scene (ex: leave the room, A looks toward B, are seated together at a table, etc)\n","- Character A and Character B are both present in a scene\n","\n","Start by calculating the ARI (adjusted Rand index) of your results. $$ ARI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n","\n","- n: number of nodes\n","- TP: True positive the number of pairs of elements that are in the same community in your results and in the ground truth\n","- TN: True negative the number of pairs of elements that are in different communities in your results and in the ground truth\n","- FP: False positive the number of pairs of elements which are in the same community in your results but which are in different communities in the ground truth\n","- FN: False negative the number of pairs of elements which are in different communities in your results but which are in the same community in the ground truth\n","\n","**N.B:** What matters here is the composition of the communities you found not the names. A TP is when the node a and the node b are both in the same communities in your result and in the ground truth.\n","\n","Answer the following questions. They are guides for your analysis.\n","\n","- Does the algorithm perform well on all seasons or for some only? \n","- Explain why you obtained those results by analysing the communities from each season. Which particularities offer better results? Which hinder the algorithm?\n","\n","You can do the manipulations you want to better present your results and better support your statements."]},{"cell_type":"markdown","metadata":{"id":"ZzndRBWVSriT"},"source":["#### Résultats / Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-2UipMRXG0R"},"outputs":[],"source":["# Mettez votre code ici\n"]},{"cell_type":"markdown","metadata":{"id":"u-HMg4sUSriX"},"source":["#### Analyse / Analysis\n","Écrivez votre analyse ici / Write your analysis here"]},{"cell_type":"markdown","metadata":{"id":"r3FmllqgXG0d"},"source":["# 2. Personnages influents dans GOT / Influent character in GOT (4 points)\n","\n","##  Analyse d'un réseau social \n","Une autre analyse intéressante à faire avec un réseau social est de trouver les personnes influentes du réseau soit les personnes autour desquelles les gens du réseau se regroupent.\n","\n","Il existe des mesures qui permettent de connaître ces personnes: les mesures de centralité. **Vous devez implémenter les mesures vous-même et ne pas utilisez les implémentations de networks de ces mesures.** Pour vous aider lors de l'implémentation de ses mesures, un deuxième toy dataset vous est fourni. Il ressemble à ceci (data/picture2.png):\n","![title](data/picture2.png)\n","\n","## GOT datasets\n","La série Games of Thrones est reconnue pour tuer ses personnages importants. Nous vous demandons de vérifier cette affirmation. Pour cette partie, vous devez utiliser tous les CSV donnés avec le TP (nodes, edges et deaths). Nous voulons que vous trouviez les personnages les plus influents de chaque saison et que vous les compariez avec la liste de personnages morts durant la saison.\n","\n","---\n","\n","##  Social network analysis\n","\n","Another interesting analysis to do with a social network is to find the influential people in the network, ie the people around whom the people in the network gather.\n","\n","There are measures which make it possible to know these people: the centrality measures. **You must implement those metrics yourselves. Do not use Networks implementation for the  tp.** To help you during the implementation of those measurements, a second toy dataset is provided to you. It looks like this (data/picture2.png): ![title](data/picture2.png)\n","\n","## GOT datasets\n","The Games of Thrones series is known to kill its important characters. We ask you to verify this statement. For this part, you must use all the csv given with the TP (nodes, edges and deaths). We want you to find the most influential characters from each season and compare them with the list of dead characters during the season.\n"]},{"cell_type":"markdown","metadata":{"id":"uJxSGCnOXG0e"},"source":["## 2.1 Centralité de degré / Degree centrality (0.5 point)\n","\n","Une première mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité de degré. Elle se calcule $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n","\n","- i: un sommet dans le réseau\n","- n: le nombre de sommets\n","- degree: le nombre d'arêtes attachées au sommet\n","\n","#### Implémentation\n","1. Implémenter la fonction `calculate_degree_centrality`. Cette fonction calcule la centralité de degré pour tous les sommets du réseau et ajoute cette mesure à chaque sommet.\n","\n","Utilisez la fonction `test_degree_centrality` pour vérifier votre implémentation. Le sommet 1 devrait avoir la plus haute mesure de 0.4375.\n","\n","---\n","\n","A first simple measure to find the importance of a node in a network is the degree centrality. It is calculated $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n","\n","- i: a node in the network\n","- n: the number of nodes\n","- degree: the number of edges attached to the node\n","\n","#### Implementation\n","1. Implement the function `calculate_degree_centrality`. This function calculates degree centrality for all nodes in the network and adds this measurement to each node.\n","\n","Use the function `test_degree_centrality` to verify your implementation. The best node should be node_1 with 0.4375."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLTg0prlSriZ"},"outputs":[],"source":["def calculate_degree_centrality(social_network):\n","    #TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad3lK2B8Srie"},"outputs":[],"source":["def test_degree_centrality():\n","    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n","    calculate_degree_centrality(social_network)\n","    dict_centrality = nx.get_node_attributes(social_network, 'degree_centrality')\n","    best_node = max(dict_centrality, key=dict_centrality.get)\n","    print(\"Highest degree centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n","test_degree_centrality()"]},{"cell_type":"markdown","metadata":{"id":"fhPWALjLSrig"},"source":["## 2.2 Centralité de proximité / Closeness centrality (0.5 point)\n","\n","Une autre mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité de proximité. Elle se calcule $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n","\n","- i: un sommet dans le réseau\n","- AvDist: la moyenne de toutes les distances les plus courtes pour atteindre chaque sommet à partir du sommet i\n","\n","#### Implémentation\n","1. Implémenter la fonction `calculate_closeness_centrality`. Cette fonction calcule la centralité de proximité pour tous les sommets du réseau et ajoute cette mesure à chaque sommet. Considérer chaque arête comme une distance de 1.\n","\n","**NB**: Utiliser la fonction `shortest_path()` du module Networkx pour trouver le chemin le plus court entre des sommets\n","\n","Utilisez la fonction `test_closeness_centrality` pour vérifier votre implémentation. Le sommet 3 devrait avoir la plus haute mesure de 0.41.\n","\n","---\n","\n","Another simple measure for finding the importance of a node in a network is closeness centrality. It is calculated $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n","\n","- i: a node in the network\n","- AvDist: the average of all shortest distances to reach each vertex from vertex i\n","\n","#### Implementation\n","1. Implement the function `calculate_closeness_centrality`. This function calculates closeness centrality for all nodes in the network and adds this measurement to each node. Consider each edge as a distance of 1.\n","\n","**NB**: Use the fucntion `shortest_path()` from Networkx to find the shortest path between two nodes.\n","\n","Use the function `test_closeness_centrality` to verify your implementation. The best node should be node_3 with 0.41."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"534teZVYSrii"},"outputs":[],"source":["def calculate_closeness_centrality(social_network):\n","    #TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVQG0XiFSril"},"outputs":[],"source":["def test_closeness_centrality():\n","    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n","    calculate_closeness_centrality(social_network)\n","    dict_centrality = nx.get_node_attributes(social_network, 'closeness_centrality')\n","    best_node = max(dict_centrality, key=dict_centrality.get)\n","    print(\"Highest closeness centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n","\n","test_closeness_centrality()"]},{"cell_type":"markdown","metadata":{"id":"OHTdy58OSrir"},"source":["## 2.3 Centralité d'intermédiarité / Betweeness centrality (1 point)\n","\n","Une dernière mesure simple pour trouver l'importance d'un sommet dans un réseau est la centralité d'intermédiarité. Elle se calcule $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n","\n","- n: le nombre de sommets dans le réseau\n","- i: un sommet dans le réseau\n","- j,k: deux sommets dans le réseau excluant i\n","- $f_{jk}(i)$: le nombre de chemin le plus court partant du sommet j vers un sommet k (> j) passant par le sommet i \n","\n","#### Implémentation\n","1. Implémenter la fonction `calculate_betweenness_centrality`. Cette fonction calcule la centralité d'intermédiarité pour tous les sommets du réseau et ajoute cette mesure à chaque sommet.\n","\n","Utilisez la fonction `test_betweennes_centrality` pour vérifier votre implémentation. Le sommet 4 devrait avoir la plus haute mesure de 0.57.\n","\n","---\n","\n","A final simple measure to find the importance of a node in a network is the betweeness centrality. It is calculated $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n","\n","- n: the number of nodes in the network\n","- i: a node in the network\n","- j,k: two nodes in the network excluding i\n","- $f_{jk}(i)$: the  number of shortest paths from vertex j to vertex k (> j) passing through node i\n","\n","#### Implementation\n","1. Implement the function `calculate_betweenness_centrality`.This function calculates the betweenness centrality for all the nodes of the network and adds this measurement to each node.\n","\n","Use the function `test_betweennes_centrality` to verify your implementation. The best node should be the node_4 with 0.57.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_D2n5C0JSris"},"outputs":[],"source":["def calculate_betweenness_centrality(social_network):\n","    #TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGVYdrAOSriu"},"outputs":[],"source":["def test_betweenness_centrality():\n","    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n","    calculate_betweenness_centrality(social_network)\n","    dict_centrality = nx.get_node_attributes(social_network, 'betweenness_centrality')\n","    best_node = max(dict_centrality, key=dict_centrality.get)\n","    print(\"Highest betweenness centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n","\n","test_betweenness_centrality()"]},{"cell_type":"markdown","metadata":{"id":"996_M0-sXG0w"},"source":["## 2.4 Analyse de vos résultats / Analysis of your results (2 points)\n","\n","Executez les trois fonctions sur les réseaux de chaque saison et présentez le top 10 pour chaque mesure. **Pour les saisons 2, 4 et 6 ne calculez pas la centralité de proximité**, car ce sont des graphes déconnectés. Pour chaque saison, comparez le top 10 des mesures avec la liste de morts de la saison disponible dans les csv death. Répondez aux questions suivantes. Elles sont des pistes de réflexions pour votre analyse.\n","\n","- Est-ce que le top 10 est suffisant pour trouver les morts importants de chaque saison? \n","- Quelle mesure semble mieux prédire les morts? \n","- Est-ce que la réputation de Games of Thrones de tuer plusieurs de ses personnages importants est fondée?\n","\n","**N.B.:** Si vous ne connaissez pas la série et vous n'êtes pas sûrs quels morts peuvent être considérés importants, faites une recherche Google sur les personnages importants. Mentionnez votre démarche et la conclusion de vos recherches. Il n'y a pas une liste précise de morts importants. Évidemment si vous me dite que Daenerys n'est pas importante, je vais douter de vos recherches. Le but est de voir votre travail de réflexion et d'analyse des mesures de centralité. \n","\n","---\n","\n","Run the three functions on the networks of each season and present the top 10 for each metric. **For season 2, 4 and 6 do not calculate the proximity centrality** because they are disconnected graph. For each season, compare the top 10 metrics with the season's death list in the death csv. Answer the following questions. They are guide for your analysis.\n","\n","- Is the top 10 enough to find the significant deaths of each season? \n","- What measure seems to better predict the dead? \n","- Is the reputation of Games of Thrones for killing many important characters founded?\n","\n","**N.B:** If you don't know the series and aren't sure which deaths are considered important, do a Google research on the important characters in the series. Metion your research and the conclusion of it. There isn't a precise list of important deaths but if you tell me that Daenerys isn't important, I will doubt of the seriousness of your research. The goal is to see how your analyse the results giving by centrality metrics."]},{"cell_type":"markdown","metadata":{"id":"y_S1YvI0Sriy"},"source":["### Résultats / Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtYPFnJ3Sri2"},"outputs":[],"source":["# Mettez le code pour présenter les résultats ici"]},{"cell_type":"markdown","metadata":{"id":"DeeyaJnKnDGB"},"source":["### Analyse / Analysis\n","\n","Écrivez ici"]},{"cell_type":"markdown","metadata":{"id":"fi6efh01UjEU"},"source":["# 3. DeepWalk (6 points)\n","\n","##  Prédiction de lien \n","Une autre analyse intéressante à faire avec un réseau social est la prédiction de liens. En regardant les liens actuels, il est possible de déterminer la probabilité qu'un lien apparaisse entre deux personnes dans le réseau. Pour ce TP, on vous demande d'implémenter le modèle de DeepWalk pour transformer les informations contenues dans le réseau et utiliser les **embeddings** pour prédire des liens.\n","\n","## DeepWalk\n","Le modèle de DeepWalk est basé sur l'idée de traiter un réseau social comme un texte. On peut ainsi utiliser des techniques d'apprentissage machine pour du traitement de language. C'est un modèle en deux étapes. La première étape est de contruire le \"dictionnaire\" du réseau en explorant le voisinage de chaque sommet. La deuxième étape applique l'algorithme SkipGram sur le \"dictionnaire\" pour apprendre les *embeddings* pertinents. Le réseau est maintenant transformé et prêt à être traité par des techniques de NLP.\n","\n","## GOT datasets\n","Nous vous demandons de choisir une saison de GoT excluant les **saisons 2, 4 et 6** pour tester votre implémentation du modèle. Enlever un lien fort, un lien moyen et un lien faible d'un sommet du réseau. Nous voulons voir si le modèle est capable de retourver ces 3 liens pour le sommet.\n","\n","---\n","##  Link prediction\n","Another interesting graph manipulation is link prediction. By looking at current links between nodes, the probability of connecting two nodes can be predicted. In this part of the TP, we want you to implement the DeepWalk model to transform the information in the graph to be used with NLP models to predict links.\n","\n","## DeepWalk\n","The DeepWalk model is based on the idea that social network can be treated like a text. As such, NLP techniques can be used to mine a social network. It is a 2 step model. The first step builds the network's dictionnary by exploring the network. The second step use the SkipGram algorithm to transform the dictionnary in embeddings. The network is then ready to be used by NLP techniques.\n","\n","## GOT datasets\n","We ask you to choose a GOT season excluding **season 2, 4 and 6** to test your implementation. Remove a strong, medium and low link from one node. We want to see if the model can find those links again.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCVIF3ngSeby"},"outputs":[],"source":["import random as rnd\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import math\n","\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum()\n","\n","\n","class DeepWalk:\n","    def __init__(self, graph):\n","        self.graph = graph\n","\n","        self.random_walks = []\n","        self.walk_length = None\n","        self.n_walk = None\n","\n","        self.node_index = None\n","        self.training_data = None\n","\n","        self.n_neurons = None\n","        self.epochs = None\n","        self.embeddings = None\n","        self.W1 = None\n","        self.loss = 0\n","        self.alpha = 0.00001\n","        self.init = None\n","\n","    def random_walk(self, start_node):\n","        # TODO\n","\n","    def build_random_walks_matrix(self, n_walk, walk_length):\n","        # TODO\n","\n","    def create_training(self, window_size):\n","        # TODO\n","\n","    def neural_network(self, epochs, n_neurons):\n","        self.n_neurons = n_neurons\n","        self.epochs = epochs\n","        self.init = math.sqrt(6 /\n","                              (self.graph.number_of_nodes() + self.n_neurons))\n","        self.embeddings = np.random.uniform(\n","            -self.init, self.init,\n","            (self.graph.number_of_nodes(), self.n_neurons))\n","        self.W1 = np.random.uniform(\n","            -self.init, self.init,\n","            (self.n_neurons, self.graph.number_of_nodes()))\n","\n","        for x in range(1, epochs):\n","            self.loss = []\n","            for x_data, y_data in self.training_data:\n","                self._feed_forward(x_data)\n","                self._backpropagate(x_data, y_data)\n","                C = 0\n","                sub_loss = 0\n","                for m in range(self.graph.number_of_nodes()):\n","                    if y_data[m]:\n","                        sub_loss += -1 * self.u[m][0]\n","                        C += 1\n","                sub_loss += C * np.log(np.sum(np.exp(self.u)))\n","                self.loss.append(sub_loss)\n","            self.loss = np.mean(self.loss)\n","            print(\"epoch \", x, \" loss = \", self.loss)\n","            self.alpha *= 1 / (1 + self.alpha * x)\n","\n","    def skip_gram(self, window_size, epochs, n_neurons):\n","        self.create_training(window_size)\n","        self.neural_network(epochs, n_neurons)\n","        return self.embeddings, self.W1\n","\n","    def _feed_forward(self, X):\n","        self.h = np.dot(self.embeddings.T, X).reshape(self.n_neurons, 1)\n","        self.u = np.dot(self.W1.T, self.h)\n","        self.y = softmax(self.u)\n","        return self.y\n","\n","    def _backpropagate(self, x, t):\n","        # e is a vector V x 1\n","        e = self.y - np.asarray(t).reshape(self.graph.number_of_nodes(), 1)\n","        dLdW1 = np.dot(self.h, e.T)\n","        X = np.array(x).reshape(self.graph.number_of_nodes(), 1)\n","        dLdW = np.dot(X, np.dot(self.W1, e).T)\n","        self.W1 = self.W1 - self.alpha * dLdW1\n","        self.embeddings = self.embeddings - self.alpha * dLdW\n"]},{"cell_type":"markdown","metadata":{"id":"4ZoPSKxISQar"},"source":["## 3.1 Random-walk (2 points)\n"," \n","La première étape du modèle DeepWalk consiste à parcourir de manière aléatoire le réseau pour se construire un \"dictionnaire\" du réseau. La marche aléatoire dépend de deux paramètres: le nombre de marche(n_walk) et la longueur de la marche(walk_length). La longeur de la marche correspond au nombre de sommets dans la marche. En explorant le graphe, le modèle se construit des \"contextes\" à partir des liens entres les sommets. On peut considérer cela comme des phrases ou des extraits de phrases dans un texte. Deux sommets sont similaires si leur contexte le sont aussi.\n"," \n"," \n","Implémentation\n","1. Implémenter la fonction `random_walk`. Cette fonction retourne une marche aléatoire à partir du sommet donné en paramètre. **N.B:** considérer que les attributs `self.n_walk` et `self.walk_length` ont déjà été initialisé avec les bonnes valeurs.\n","2. Implémenter la fonction `build_random_walks_matrix`. Cette fonction trouve le voisinage de tous les sommets du réseau. Cela génère donc une matrice de taille (nbre_sommet * n_walk) x walk_length, car chaque sommet a n_walk marches aléatoires. Cette matrice est contenu dans `self.random_walks`.\n","\n","Pour augmenter la composante aléatoire de la matrice, l'ordre des sommets doit aussi être aléatoire. Il ne faut pas que les n_walk premières rangées contiennent toutes les marches pour le même sommet du graphe ou que les marches suivent toujours le même ordre de sommets. \n","\n","---\n","The first step in the DeepWalk model is to create the dictionary by exploring the network randomly. The random walk depends on two parameters: the number of walks(`n_walk`) the length of the walk(`walk_length`). The length of the walk is the number of node in the walk. By exploring the graph, the model builds context with the links between nodes. Those links make up \"sentences\". Nodes are similar if their context are similar.\n","\n","Implementation\n","\n","1. Implement the function `random_walk`. This function returns a random walk starting with the node `start_node`. N.B: consider that self.n_walk and self.walk_length are already initialize with the right values.\n","2. Implement the function `build_random_walks_matrix`. This function finds the neighborhood for all the nodes in the network. This generates a (`n_nodes` * `n_walk`) x `walk_length` matrix. Each node has n_walk random walks. The matrix is stored in `self.random_walks`.\n","\n","To ensure randomness in the matrix, the order of the nodes have to be random too. The n_walk first rows can't contains walk from the same node or that the walks always follow the same node order.\n"]},{"cell_type":"markdown","metadata":{"id":"Q8DPJqFcsAtc"},"source":["## 3.2 SkipGram (1 points)\n"," \n","La deuxième étape consiste à transformer le \"dictionnaire\" en *embedding*.\n","\n","1. Commencer par créer l'ensemble d'entrainement à partir des marches aléatoires crées précédemment en implémentant la fonction `create_training`. Pour créer l'ensemble, il faut itérer à travers chaque sommet de chaque marche. Le sommet actuel est considéré comme le target. Il servira d'input. Son contexte/voisinage servira à vérifier la prédiction. Ce contexte correspond au `window_size` sommets avant et après le sommet actuel dans la marche. Ces paires de target/contexte sont conservés dans `self.training_data`.  Chaque  ligne contient un target et son contexte. Pour faciliter l'utilisation de ses informations un encoding one-hot est utilisé. Le target est donc un vecteur avec un 1 à l'index du sommet actuel. Le contexte est donc un vecteur avec des 1+(au cas où un sommet se retrouve plusieurs fois dans le voisinage) à l'index des sommets voisins. Le vocabulaire considéré pour l'encoding est l'ensemble des sommets dans le réseau. \n","\n","La fonction `skip_gram` est implémentée pour vous. Cette fonction utilise l'ensemble d'entrainement pour trouver les embedding des sommets. Pour faire cela, le modèle envoie chaque target dans un réseau de neurones à une couche et fait un sofmax sur le résultat pour comparer le résultat avec le contexte. Le réseau de neurone est implémenté dans la fonction `neural_network`.\n","\n","---\n","\n","The second step is to transform the dictionary in embedding.\n","\n","1. Start by creating a training set from the random walks created previously in the function `create_training_set`. Each node in each walk is considered as a target. The context/neighborhood is the `window_size` nodes before and after the target. The target is used as an input while the context is used as the groundtruth for the prediction. Those pairs of target/context are stored in `self.training_data`. Each row has the target followed by its context. That information is encoded with one-hot encoding. The target is then a vector with a 1 in the index of the actual node. The context is a vector with 1+ (for cases where a node appears multiple times in the neighborhood) in the index of the neighboring nodes. The vocabulary used for the encoding is all the nodes in the network.\n","\n","The function `skip_gram` is implemented for you. This function uses the training set to find the nodes' embedding. The model gives each target to the neural network and uses a softmax on the results to compare it with the context. The model then backpropagate the error to correct the embedding. The neural network is in the fonction `neural_network`."]},{"cell_type":"markdown","metadata":{"id":"EVihLggbF-4V"},"source":["## 3.3 Prédiction de liens / Link prediction (3 points)\n"," \n","La dernière étape consiste à utiliser les *embedding* pour faire de la prédiction de lien.\n","\n","Nous vous demandons de choisir une saison de GoT excluant les **saisons 2, 4 et 6** pour tester votre implémentation du modèle. Enlevez un lien fort, un lien moyen et un lien faible d'un sommet du réseau. Utilisez la valeur de `weight` dans le csv de edge pour déterminer la force du lien. Après avoir enlevé les 3 liens, appliquez DeepWalk sur le graph résultant et utilisez les *embeddings* pour prédire des liens sur le sommet. Nous voulons voir si le modèle est capable de retourver ces 3 liens pour le sommet. Les embeddings sont dans self.embeddings. Chaque rangée correspond aux embeddings pour un sommet dans le réseau. Votre modèle devra prendre deux vecteurs et prédire 1 s'il y a un lien et 0 s'il n'y en a pas. **Indice**: deux vectors proches ont plus de chance d'être liés.\n","\n","Montrez et discutez des résultats obtenus. Répondez aux questions suivantes. Elles vous serviront de guides pour votre réflexion.\n","\n","- Expliquez votre modèle et pourquoi vous l'avez choisi.\n","- Avez-vous réussi à trouver les 3 liens facilement?\n","- Trouvez-vous des liens non-existants dans le graph de la saison actuelle?\n","- Quels sont les impacts des paramètres sur vos résultats?\n","\n","---\n","\n","The last step is to use the embeddings to predict new links.\n","\n","We ask you to choose a GoT season **excluding the season 2, 4 and 6** to test your implementation. Remove a strong, medium and weak link from a node in the network. Use the `weight` value in the edge csv to check the links' strength. After removing the 3 links, apply the DeepWalk model on the resulting network and try to predict those 3 links. The embeddings are stored in self.embedding. Each row is the embedding for a node in the network. Your  model should take two vectors and output 1 for a link and 0 if not. **Hint**: linked vectors would be close with eact other.\n","\n","Show and discuss your result. Answer the following questions. They are guides for your reflexion.\n","\n","- Explain your model and why you chose it.\n","- Did you find all 3 links?\n","- Did you find links present in other seasons but not in this one?\n","- How did the parameter impacts your results?\n"]},{"cell_type":"markdown","metadata":{"id":"G0p1nqLgpnw9"},"source":["### Résultats / Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLLU3d5qGhSg"},"outputs":[],"source":["# code pour votre prédiction ici"]},{"cell_type":"markdown","metadata":{"id":"4eG-a3IlnI0G"},"source":["### Analyse / Analysis\n","Écrivez votre analyse ici"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"TP3_INF8111.ipynb","provenance":[]},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3.9.7 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
